{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip \n",
    "import ast\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_json2 = []\n",
    "# Abrir el archivo y leer línea por línea\n",
    "with gzip.open('../json/user_reviews.json.gz', 'rt', encoding= 'utf-8') as f2:\n",
    "    for li in f2:\n",
    "        json_linea1 = ast.literal_eval(li)\n",
    "        # Agregar el objeto JSON decodificado a la lista\n",
    "        datos_json2.append(json_linea1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(datos_json2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25799 entries, 0 to 25798\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   user_id   25799 non-null  object\n",
      " 1   user_url  25799 non-null  object\n",
      " 2   reviews   25799 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 604.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(reviews_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.dropna(how='all', inplace=True) # 25799 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para desanidar las reseñas\n",
    "def desanidar_reseñas(row):\n",
    "    user_id = row['user_id']\n",
    "    user_url = row['user_url']\n",
    "    reseñas = row['reviews']\n",
    "    result = []\n",
    "    for idx, res in enumerate(reseñas, start=1):\n",
    "        res['user_id'] = user_id\n",
    "        res['user_url'] = user_url\n",
    "        res['review_id'] = f'{user_id}_review_{idx}'\n",
    "        result.append(res)\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dsn = pd.concat(reviews_df.apply(desanidar_reseñas, axis=1).tolist(), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "umbral_negativo = -0.1\n",
    "umbral_positivo = 0.1\n",
    "\n",
    "\n",
    "for index, row in reviews_dsn.iterrows():\n",
    "    if pd.notnull(row['review']):\n",
    "        sentiment_score = sia.polarity_scores(row['review'])['compound']\n",
    "\n",
    "        if sentiment_score < umbral_negativo:\n",
    "            reviews_dsn.at[index, 'feeling'] = 0\n",
    "        elif sentiment_score > umbral_positivo:\n",
    "            reviews_dsn.at[index, 'feeling'] = 2\n",
    "        else:\n",
    "            reviews_dsn.at[index, 'feeling'] = 1\n",
    "    else:\n",
    "        reviews_dsn.at[index, 'feeling'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dsn['posted'] = reviews_dsn['posted'].str.replace('Posted', '')\n",
    "reviews_dsn['posted'] = reviews_dsn['posted'].str.replace('.', '')\n",
    "reviews_dsn['posted'] = reviews_dsn['posted'].str.replace(',', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_fecha(date_string):\n",
    "    try:\n",
    "        # Intentar parsear la cadena en un objeto de fecha\n",
    "        date_string = date_string.rstrip('.')\n",
    "        fecha_convertida = parser.parse(date_string, default=datetime.datetime(2000, 1, 1))\n",
    "    except ValueError:\n",
    "        # Si hay un error, imprimir el mensaje de error y devolver None\n",
    "        print(f\"No se puede convertir la fecha: {date_string}.\")\n",
    "        return None\n",
    "    return fecha_convertida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dsn['posted'] = reviews_dsn['posted'].apply(convertir_fecha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_dsn['item_id'] = reviews_dsn['item_id'].astype(int)\n",
    "reviews_dsn = reviews_dsn[reviews_dsn['item_id'] != 440]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(reviews_dsn)\n",
    "pq.write_table(table, \"../data_transformed/reviews.parquet\", compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pq.read_table(\"../data_transformed/reviews.parquet\")\n",
    "user_reviews_df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recommend\n",
       "True     6427\n",
       "False    2820\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = user_reviews_df[user_reviews_df['feeling'] == 0]\n",
    "b = a['recommend'].value_counts()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Luka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews_dsn['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_review (review):\n",
    "    review = review.lower()\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \", str(review)) # Elimina los caracteres que no sean letras o signos de exclamación\n",
    "    review_wt = nltk.tokenize.word_tokenize(review) # Tokeniza y pasa a minúsculas el texto\n",
    "    review_wt = [word for word in review_wt if word not in stopwords] # Elimina las stopwords\n",
    "    review_wt = [lemmatizer.lemmatize(word) for word in review_wt]  # Aplicamos la funcion para buscar la raiz de las palabras\n",
    "    review_wt = \" \".join(review_wt) # Por ultimo volvemos a unir el titular\n",
    "    return review_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = []\n",
    "for review in reviews:\n",
    "    review_wt = preproc_review(str(review))\n",
    "    reviews_list.append(review_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Inicializar el vectorizador CountVectorizer para bolsa de palabras\u001b[39;00m\n\u001b[0;32m      4\u001b[0m vectorizador_bolsa_palabras \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m750\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m , ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Inicializar el vectorizador CountVectorizer para bolsa de palabras\n",
    "vectorizador_bolsa_palabras = CountVectorizer(max_features=750, stop_words=\"english\" , ngram_range=(1,2))\n",
    "\n",
    "# Ajustar y transformar los textos en una matriz de características\n",
    "matriz_caracteristicas_bolsa_palabras = vectorizador_bolsa_palabras.fit_transform(reviews_list)\n",
    "\n",
    "# Obtener el vocabulario (tokens únicos)\n",
    "vocabulario_bolsa_palabras = vectorizador_bolsa_palabras.get_feature_names_out()\n",
    "\n",
    "# Imprimir la matriz de características y el vocabulario\n",
    "print(\"Matriz de características (Bolsa de palabras):\\n\", matriz_caracteristicas_bolsa_palabras.toarray())\n",
    "print(\"Vocabulario (Tokens únicos):\", vocabulario_bolsa_palabras)\n",
    "\n",
    "# # Inicializar el vectorizador TfidfVectorizer para TF-IDF\n",
    "# vectorizador_tfidf = TfidfVectorizer(max_features=1000, stop_words=\"english\" , ngram_range=(1,2))\n",
    "\n",
    "# # Ajustar y transformar los textos en una matriz de características TF-IDF\n",
    "# matriz_caracteristicas_tfidf = vectorizador_tfidf.fit_transform(reviews.tolist())\n",
    "\n",
    "# # Imprimir la matriz de características TF-IDF\n",
    "# print(\"\\nMatriz de características (TF-IDF):\\n\", matriz_caracteristicas_tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
